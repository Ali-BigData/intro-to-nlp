{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP\n",
    "\n",
    "Themes:\n",
    "1. What is Natural Language Processing (NLP) / Language Technology?\n",
    "2. Getting textual data\n",
    "3. Text Segmentation\n",
    "4. Word Frequencies\n",
    "5. Stemming and Lemmatization\n",
    "\n",
    "## 1. Natural Language Processing / Language Technology\n",
    "\n",
    "* Natural language = human language\n",
    "    * Compared to artificial languages, for example programming languages\n",
    "    * **variety and ambiguity:** the same meaning can be expressed many different ways (variety), and the same surface realization can express many different meanings based on context (ambiguity)\n",
    "    \n",
    "![human_language.png](figs/human_language.png)\n",
    "    \n",
    "* NLP means different computer-based methods to analyze, understant or generate human language\n",
    "* Wikipedia: \"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"\n",
    "\n",
    "## NLP Applications\n",
    "\n",
    "### Simple counting\n",
    "* How many words/characters an essay have?\n",
    "* Which are the most frequently used words in Finnish?\n",
    "\n",
    "### Spell Checker\n",
    "* Highlight spelling errors in text editors\n",
    "* Auto correction in mobile phones\n",
    "\n",
    "### Search Engines\n",
    "* Google, Bing, Baidu\n",
    "\n",
    "### Speech Recognition / Assistant\n",
    "* Speech-to-text\n",
    "* Apple Siri, Google Now\n",
    "\n",
    "### Machine Translation\n",
    "![machine_translation.png](figs/machine_translation.png)\n",
    "\n",
    "### Text Generation\n",
    "![talktotransformer.png](figs/talktotransformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Textual Data\n",
    "\n",
    "* Data is everywhere you can see written or spoken language\n",
    "* **Corpus** is a structured and documented collection of data\n",
    "    * Text Corpus = A collection of written text\n",
    "    * Speech corpus = A collection of spoken language (audio)\n",
    "* Preferably in an easily machine readable format\n",
    "* Raw text: A collection of unprocessed text\n",
    "![raw_text.png](figs/raw_text.png)\n",
    "* Annotated text corpora: A collection of text with markings\n",
    "    * For each document/sentence, mark its language or topic\n",
    "    * For each word, mark its part-of-speech category\n",
    "    * etc.\n",
    "    * human or machine generated\n",
    "\n",
    "![annotated_text.png](figs/annotated_text.png)\n",
    "![labeled_text.png](figs/labeled_text.png)\n",
    "\n",
    "* Corpus properties\n",
    "    * Language\n",
    "    * Size\n",
    "    * Structure (E.g. Do we store full documents or individual sentences)\n",
    "    * Domain (Do we store certain type of documents, e.g. only news articles, or any text documents)\n",
    "        * Closed-domain, e.g. news corpus\n",
    "        * Open-domain, anything and everything, no boundaries\n",
    "    \n",
    "### Textual Data / Corpora sources: \n",
    "* **Web pages**\n",
    "    * crawler = internet bot that systematically browses the www by following hyperlinks\n",
    "        * Start with a list of seed URLs --> While visiting an URL, identify all hyperlinks and add them to the list of URLs to visit --> Continue until the list of URLs to visit is empty\n",
    "    * Web crawling / web scraping = save all data while browsing the www\n",
    "    * Remember: You need to be polite!\n",
    "        * Be aware of the local rules\n",
    "        * Tell who you are and why you are crawling\n",
    "        * Crawl delay!!!\n",
    "        * robots.txt\n",
    "* **News papers** (online or scanned paper versions)\n",
    "* **Discussion forums**\n",
    "* **Twitter**\n",
    "    * Crawling Twitter is super easy, has API and ready-made libraries to access it\n",
    "        * You need Twitter account, create an app and identification tokens for it\n",
    "        * Sign in developer.twitter.com → click apps → create an app → go to Keys and Tokens → click create\n",
    "        * To create new apps, you need to apply for a developer account or be invited to an existing educational team\n",
    "    * How to download tweets: https://github.com/jmnybl/notebook-examples/blob/master/twitter_dl.ipynb\n",
    "    \n",
    "![twitter_interface.png](figs/twitter_interface.png)\n",
    "![twitter_json.png](figs/twitter_json.png)\n",
    "    \n",
    "\n",
    "* **A lot of ready made (raw text and annotated) corpora exists!**\n",
    "    * Wikipedia: All Wikipedia articles for a language as one downloadable file (https://dumps.wikimedia.org/)\n",
    "    * Ready-made corpora for Finnish language: Kielipankki (https://www.kielipankki.fi/aineistot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example corpus: IMDB Dataset\n",
    "\n",
    "* IMDB Dataset (English)\n",
    "    * A collection of IMDB movie reviews with a labeling to positive and negative reviews\n",
    "    * Negative review: Low score --> bad movie\n",
    "    * Positive review: High score --> good movie\n",
    "    \n",
    "* Let's inspect how to read the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'list'>\n",
      "First item type: <class 'dict'>\n",
      "First item: {'class': 'pos', 'text': \"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.  Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.  The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.  Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.  Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"}\n"
     ]
    }
   ],
   "source": [
    "import json # JSON encoder and decoder: store python data structures (e.g. lists and dictionaries) as text files\n",
    "\n",
    "with open(\"Data/imdb_train.json\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(\"Data type:\", type(data))\n",
    "print(\"First item type:\", type(data[0]))\n",
    "print(\"First item:\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 25000\n",
      "25000\n",
      "With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.  Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.  The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.  Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.  Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\n"
     ]
    }
   ],
   "source": [
    "# How many documents the dataset have?\n",
    "print(\"Number of documents:\", len(data))\n",
    "\n",
    "documents = [document[\"text\"] for document in data] # right now we only need the text field for each document\n",
    "print(len(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Segmentation\n",
    "\n",
    "* **Segmentation:** Divide bigger units into smaller ones\n",
    "* In many cases text needs to be segmented into sentences and/or words\n",
    "* Why?\n",
    "\n",
    "\n",
    "* **Tokenization / word segmentation:** Segment text into individual tokens\n",
    "* **Sentence splitting / sentence segmentation:** Segment text into individual sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremely bad customer service\n",
    "\n",
    "Do not go to this salon, especially if you have to get your hair straightened. They did a very bad job with my hair and were extremely rude when I went back to ask them why it didn't work for my hair. Rude, insensitive, discourteous people!!!!!\n",
    "\n",
    "(text source: https://github.com/UniversalDependencies/UD_English-EWT)\n",
    "\n",
    "**Tokenized:**\n",
    "Extremely bad customer service\n",
    "\n",
    "Do not go to this salon , especially if you have to get your hair straightened . They did a very bad job with my hair and were extremely rude when I went back to ask them why it did n't work for my hair . Rude , insensitive , discourteous people !!!!!\n",
    "\n",
    "**Sentence splitted:**\n",
    "Extremely bad customer service\n",
    "\n",
    "Do not go to this salon, especially if you have to get your hair straightened.\n",
    "\n",
    "They did a very bad job with my hair and were extremely rude when I went back to ask them why it didn't work for my hair.\n",
    "\n",
    "Rude, insensitive, discourteous people!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization: HOW?\n",
    "\n",
    "* **Naive method 1:** Split from whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely\n",
      "bad\n",
      "customer\n",
      "service\n",
      "Do\n",
      "not\n",
      "go\n",
      "to\n",
      "this\n",
      "salon,\n",
      "especially\n",
      "if\n",
      "you\n",
      "have\n",
      "to\n",
      "get\n",
      "your\n",
      "hair\n",
      "straightened.\n",
      "They\n",
      "did\n",
      "a\n",
      "very\n",
      "bad\n",
      "job\n",
      "with\n",
      "my\n",
      "hair\n",
      "and\n",
      "were\n",
      "extremely\n",
      "rude\n",
      "when\n",
      "I\n",
      "went\n",
      "back\n",
      "to\n",
      "ask\n",
      "them\n",
      "why\n",
      "it\n",
      "didn't\n",
      "work\n",
      "for\n",
      "my\n",
      "hair.\n",
      "Rude,\n",
      "insensitive,\n",
      "discourteous\n",
      "people!!!!!\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"Extremely bad customer service\n",
    "\n",
    "Do not go to this salon, especially if you have to get your hair straightened. \\\n",
    "They did a very bad job with my hair and were extremely rude when I went back to \\\n",
    "ask them why it didn't work for my hair. Rude, insensitive, discourteous people!!!!!\"\"\"\n",
    "\n",
    "tokenized_text = text.split() # split(): Return a list of the words in the string, using whitespace as the delimiter string.\n",
    "\n",
    "for w in tokenized_text:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Naive method 2:** Split from whitespace characters, take into account punctuation\n",
    "* Regular expressions!\n",
    "    * Define search patters\n",
    "    * Find this kind of pattern from raw text, or find-and-replace\n",
    "* Find all punctuation characters, and replace with whitespace+punctiation character\n",
    "    * *book.* --> *book .*\n",
    "    * *people!!!!!* --> *people !!!!!*\n",
    "    * How about clitics in English? [don't, can't, cannot?]\n",
    "    * 2-(14-hydroxypentadecyl)-4-methyl-5-oxo-2,5-dihydrofuran-3-carboxylic acid ???\n",
    "    * Usually it's not that important how exactly you do it, just be consistent!\n",
    "        * consistent = always do it the same way\n",
    "        * If you download two datasets which are already tokenized, the tokenization may differ and you need to be aware of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely bad customer service\n",
      "\n",
      "Do not go to this salon , especially if you have to get your hair straightened . They did a very bad job with my hair and were extremely rude when I went back to ask them why it did n't work for my hair . Rude , insensitive , discourteous people !!!!!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokenized = re.sub(r'([.,!?]+)', r' \\1', text) # replace . , ! ? with whitespace+character(s), '+' means one or more\n",
    "tokenized = re.sub(r\"(n't)\", r\" \\1\", tokenized) # clitics\n",
    "\n",
    "print(tokenized) # Note: this is still string, apply simple whitespace splitting to get a list of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Naive method 2** works quite well for English, Finnish, Swedish...\n",
    "    * Approx. 97-99% correct on clean text\n",
    "    * Many existing tokenizers are just (a bunch of) regular expressions\n",
    "    * Can be hundreds of different rules...\n",
    "\n",
    "\n",
    "* How about other languages, does it work for all?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "**Nope! Why not?**\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "* All languages do not use whitespace or punctuation, or the meaning of those may be different.\n",
    "* Chinese, Thai, Vietnamese\n",
    "\n",
    "![tokenization.png](figs/tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Naive algorithm:**\n",
    "    1) Build a vocabulary for the language\n",
    "    2) Start from the beginning of the text, and find the longest matching word\n",
    "    3) Split the matching word and continue from the next remaining character\n",
    "* *the table down there* --> *thetabledownthere* --> *theta bled own there*\n",
    "    * Does not work well for English, but in Chinese words are usually 2-4 characters long, so the simple algorithm works better\n",
    "    \n",
    "**Tokenization: State-of-the-art**\n",
    "* State-of-the-art = The best existing method currently known\n",
    "* Machine learning\n",
    "    * Collect raw (untokenized) text for the language you are interested in, and manually tokenize it.\n",
    "    * Train a classifier\n",
    "    * The trained classifier can be used to tokenize new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence splitting: HOW?\n",
    "\n",
    "* **Naive method 1:** What kind of punctuation characters end the sentence?\n",
    "    * yes: . ! ?\n",
    "    * no: ,\n",
    "* Define a list of sentence-final punctuation, and always split on those.\n",
    "* Problems?\n",
    "\n",
    "![sentence_splitting.png](figs/sentence_splitting.png)\n",
    "\n",
    "\n",
    "* **Solution 1:** Define a list of rules to identify when punctuation does not end a sentence\n",
    "    * List of known abbreviations, list of regular expression to regocnize numbers etc. (*The cost was approx. 1.5 million euros.*)\n",
    "    * How about missing punctuation? Other languages?\n",
    "    \n",
    "**Sentence splitting: State-of-the-art**\n",
    "* Machine learning\n",
    "    * Collect raw text for the language you are interested in, and manually sentence segment it.\n",
    "    * Train a classifier\n",
    "    * The trained classifier can be used to sentence segment new text\n",
    "    \n",
    "## Try UDPipe machine learned tokenizer and sentence splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With all this stuff going down at the moment with MJ i 've started listening to his music , watching the odd documentary here and there , watched The Wiz and watched Moonwalker again .\n",
      "Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent .\n",
      "Moonwalker is part biography , part feature film which i remember going to see at the cinema when it was originally released .\n",
      "Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay .\n",
      "Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring .\n",
      "Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him .\n",
      "The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord .\n",
      "Why he wants MJ dead so bad is beyond me .\n",
      "Because MJ overheard his plans ?\n",
      "Nah , Joe\n",
      "Pesci 's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno , maybe he just hates MJ's music .\n",
      "Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence .\n",
      "Also , the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene .\n",
      "Bottom line , this movie is for people who like MJ on one level or another ( which i think is most people ) .\n",
      "If not , then stay away .\n",
      "It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl !\n",
      "Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty ?\n",
      "Well , with all the attention i 've gave this subject .... hmmm well i do n't know because people can be different behind closed doors , i know this for a fact .\n",
      "He is either an extremely nice but stupid guy or one of the most sickest liars .\n",
      "I hope he is not the latter .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try to tokenize and sentence split the IMDB data with UDPipe machine learned segmenter!\n",
    "# Documentation: https://ufal.mff.cuni.cz/udpipe/users-manual\n",
    "# Training data: \n",
    "# Finnish (Data/fi.segmenter.udpipe): https://github.com/UniversalDependencies/UD_Finnish-TDT v.2.2\n",
    "# English (Data/en.segmenter.udpipe): https://github.com/UniversalDependencies/UD_English-EWT v.2.2\n",
    "# Swedish (Data/sv.segmenter.udpipe): https://github.com/UniversalDependencies/UD_Swedish-Talbanken v.2.2\n",
    "\n",
    "#!pip3 install ufal.udpipe\n",
    "\n",
    "import ufal.udpipe as udpipe\n",
    "\n",
    "model = udpipe.Model.load(\"Data/en.segmenter.udpipe\")\n",
    "pipeline = udpipe.Pipeline(model,\"tokenize\",\"none\",\"none\",\"horizontal\") # horizontal: returns one sentence per line, with words separated by a single space\n",
    "\n",
    "segmented_document = pipeline.process(documents[0])\n",
    "\n",
    "print(segmented_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word frequencies\n",
    "\n",
    "* Stop words\n",
    "* tf-idf\n",
    "\n",
    "## 5. Stemming and lemmatization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
