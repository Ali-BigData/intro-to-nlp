{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "## Objectives of this lecture\n",
    "\n",
    "* Introduce text classification as a task\n",
    "* Introduce basic approaches to text classification, especially the bag-of-words model\n",
    "* Show practical implementation of the concepts in the previous two lectures (intro and machine learning primer)\n",
    "* Train a simple classifier on the IMDB dataset\n",
    "\n",
    "## Purporse of these materials\n",
    "\n",
    "* These materials support, but do not replace the lectures\n",
    "* Presence on lectures highly recommended\n",
    "* Make notes!\n",
    "\n",
    "## Text classification\n",
    "\n",
    "* Text in, class label out\n",
    "* Often thought especially in terms of \"Document classification\" --- document in, class label(s) out\n",
    "  * Movie review -> star rating\n",
    "  * Email -> ham or spam\n",
    "  * News article -> list of topics from a pre-defined set of topic categories\n",
    "  * Comment in online discussion -> abusive or not\n",
    "  * Customer email / call to customer service -> topic and relevant business process\n",
    "  * Customer feedback -> needs reaction or not\n",
    "  * ...\n",
    "  \n",
    " * Document classification can be seen as a supervised classification problem with pre-defined classes\n",
    " * A direct application of the standard approach:\n",
    "   1. prepare training/dev/test data\n",
    "   2. extract features\n",
    "   3. train your favorite classifier\n",
    "   4. test on held-out data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words model\n",
    "\n",
    "* BoW is the simplest way to model documents for classification\n",
    "* The document is reduced to a **set** of features, in the simplest case the words\n",
    "\n",
    "\n",
    "* Feature vector: a vector with as many dimensions as we have unique features, and a non-zero value set for every feature present in our example\n",
    "* Values: 1/0 (present/absent) or oftentimes TF/IDF weights (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB data\n",
    "\n",
    "* Movie review sentiment positive/negative\n",
    "* Some 25,000 examples, 50:50 split of classes (why is this number relevant?)\n",
    "* Current state-of-the-art is about 95% accuracy (what is accuracy?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: pos\n",
      "text: When people nowadays hear of a 1940s drama, they usually appear to create a distance of irony claiming that it's another tearjerker with great stars in the lead of tragic, melancholic roles. This opinion, however, does not resemble Neorealist movies, in particular this one directed by Count Luchino Visconti. OSSESSIONE as his debut once censored and once cherished as nearly a realistic masterpiece is still loved by some people and strongly criticized by others. The contradictory opinions about the film that have appeared in these 65 years seem to have been caused by the content of the movie itself, exceptionally controversial for modern times as well as the past. At the same time, while being based on the novel by James M. Cain, THE POSTMAN ALWAYS RINGS TWICE, it is one of the most genuine screen adaptations where director remains his own style, view, his own art. I have seen the film twice and the second viewing led me to very detailed analysis part of which I'd like to entail below.  First, Visconti's movie seems to touch all psychology and actions that people may do in life, in particular those absorbed by desire. These people make such tragic decisions in spite of terrible consequences they are bound to face. Gino (Massimo Girotti) a traveler with \\bear like shoulders\\\" turns up at the crossroad of a motorway near Ferrara and enters the tavern. Although many people go there to have a meal, Gino occurs to get something more - much more: the indefatigable desire of beautiful Giovanna (Clara Calamai) a woman already married to an elderly man who runs the bar, Mr Giuseppe Bragana (Juan De Landa). Her body and her song possess his mind totally and from the moment of their first love, the couple plan to get rid of the old obstacle and build up a new life together... However, are people bound to wrong deeds in face of desire? Can one build love upon murder? What is love and what is loyalty? Does desire lead to a dangerous addiction or even obsession? Such questions intensely arise while watching the movie, when, to the core, the viewer is supplied with an insight into characters. \\\"We have to love each other affectionately\\\" answers Giovanna seemingly giving a cure to all crying conscience but may desirous love justify and cure everything? \\\"Isn't it what we both wanted\\\" says one of the couple... it occurs that it's not. Therefore, the content of the film appears to be very dangerous if not analyzed with intellect and heart. Yet, it constantly remains thought provoking.  Second, OSSESSIONE has a very strong point that talks to modern viewers: brilliant moments and marvelous cinematography, which go in pair with memorable sequences and visual power. These make a modern viewer realize that a film made almost 70 years ago is absolutely entertaining to watch. They range from tasteful erotic images to purely technical shots. Who can possibly skip that moment in Ferrara where Gino meets a beautiful girl, a sort of \\\"Ragazza Perfetta\\\" (perfect girl), a dancer Anita and buys her ice cream. His desires show him totally different direction... Do viewers remain indifferent at Gino-Giovanna's first meeting? The first focus of camera is on Giovanna's legs seemingly representing carnal desire over love that Gino experiences. A marvel of shot is Gino and Giovanna leaving the investigation room and the closeup of their shadows that directs our attention towards their suspicious look.  Third, OSSESSIONE can boast outstanding performances both from the leading pair as well as the supporting cast. Massimo Girotti once said in an interview that working in this movie had been one of the most difficult jobs he had ever done; yet, consequently, what comes out is a flawless acting. He portrays a bisexual man torn within desires who commits a crime but cannot stand any of the objects that remind him of his victim, which represents conscience. His bisexuality is indicated through the character of Lo Spagnolo (Elio Marcuzzo) whom he meets in very surprising circumstances in the train for Ancona. Clara Calamai, who was cast in the role after eminent Anna Magnani had refused, fits very well to the role and we may claim that there is a true chemistry between the couple. They are both very convincing. Besides, I liked Juan De Langa in the role of Bragana: he portrays an old husband not affectionate to his wife and still crazy about high art. In some of his most witty moments, he asks his wife to wash his back or walks in the empty streets singing his favorite opera songs after sort of karaoke performance.  In sum, we, as modern viewers who are capable of critical view, have to look at this film very objectively. It is art for sure thanks to the aspects aforementioned, it is a powerful story as well thanks to the controversy it carries; yet, is it educational? Visconti was not Fellini who said that he did not carry any message for humanity. In such case, his films would only entertain (which is, of course, not entirely Fellini's style, too). Visconti always had something to convey. What did he want to say here? Is the film against bad marriage? Or is it against wrong actions of people absorbed by desire? The final shocking moments say for themselves. Though you don't have to agree with the vision, OSSESSIONE is really a wonderfully realistic film, one of Visconti's best 8/10\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open(\"data-bow/imdb_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) #play it safe! (why?)\n",
    "print(\"class label:\", data[0][\"class\"])\n",
    "print(\"text:\",data[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words in practice\n",
    "\n",
    "* We will need to build a feature vector for every example\n",
    "* We will need to know the class for every example\n",
    "\n",
    "* Build a data matrix with dimensionality (number of examples, number of possible features), and a value for each feature, 0/1 for binary features, TF-IDF weights are also a typical choice\n",
    "\n",
    "It is quite useless to do all this ourselves, so we will use ready-made classes and functions mostly from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This many texts 25000\n",
      "This many labels 25000\n",
      "\n",
      "pos When people nowadays hear of a 1940s drama, they u...\n",
      "neg Oh my god! This movie insults the intelligence of ...\n",
      "neg I wasn't as \\lucky\\\" as some of the others comment...\n",
      "neg Can anybody do good CGI films besides Pixar? I mea...\n",
      "neg For romantic comedies, I often judge the quality o...\n",
      "pos This- and not a certain slightly overrated Souther...\n",
      "neg I am starting this review with a big giant spoiler...\n",
      "neg I was in second grade, 12 years ago. I remember it...\n",
      "pos You play as B.J. Blazkowicz, a US secret agent sol...\n",
      "pos I just realised I've been using IMDb for years now...\n",
      "pos I love the way that this game can make you literal...\n",
      "pos Chris and Andre are two average, ordinary teens. M...\n",
      "pos \\Twelve monkeys\\\"'s got all the elements to become...\n",
      "pos Come on people. This movie is better than 4. I can...\n",
      "pos definitely needed a little work in season 2. Such ...\n",
      "neg La Sanguisuga Conduce la Danza, or The Bloodsucker...\n",
      "neg 1st watched 11/7/2002 - 2 out of 10(Dir-John Bianc...\n",
      "neg I was not expecting much going in to this, but sti...\n",
      "pos I liked this movie. When the guy who was in on a b...\n",
      "neg After eagerly waiting to the end, I have to say I ...\n"
     ]
    }
   ],
   "source": [
    "# We need to gather the texts and labels into separate lists\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(\"This many texts\",len(texts))\n",
    "print(\"This many labels\",len(labels))\n",
    "print()\n",
    "for label,text in list(zip(labels,texts))[:20]:\n",
    "    print(label,text[:50]+\"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn text vectorizers\n",
    "\n",
    "* Vectorizers take care of turning inputs into feature vectors\n",
    "* Also build the feature name to feature index mapping:\n",
    "    * `.fit()` learn the mapping\n",
    "    * `.fit_transform()` learn and apply the mapping\n",
    "    * `.transform()` apply the learned mapping\n",
    "    * (What is the difference?)\n",
    "\n",
    "Let's first try on a tiny example, then work our way to the real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique features:\n",
      "['commodities', 'global', 'gold', 'has', 'is', 'markets', 'metal', 'more', 'of', 'on', 'palladium', 'precious', 'price', 'soared', 'soaring', 'than', 'the', 'why']\n",
      "\n",
      "Feature vectors (sparse format):\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 16)\t3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#just two short document\n",
    "toy_data=[\"More precious than gold: Why the metal palladium is soaring\",\"The price of the precious metal palladium has soared on the global commodities markets.\"]\n",
    "\n",
    "vectorizer.fit(toy_data)\n",
    "print(\"Unique features:\")\n",
    "print(vectorizer.get_feature_names())\n",
    "print()\n",
    "print(\"Feature vectors (sparse format):\")\n",
    "print(vectorizer.transform(toy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1]\n",
      " [1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 3 0]]\n"
     ]
    }
   ],
   "source": [
    "#...and in a more understandable format\n",
    "#...these are the feature vectors of our two toy documents\n",
    "print(vectorizer.transform(toy_data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# only features seen in the training data can be taken into account!\n",
    "print(vectorizer.transform([\"unseen words only\"]).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape= (25000, 74849)\n",
      "what did we get? -> <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
    "feature_matrix=vectorizer.fit_transform(texts)\n",
    "print(\"shape=\",feature_matrix.shape)\n",
    "print(\"what did we get? ->\", feature_matrix.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 39)\t1\n",
      "  (0, 7045)\t1\n",
      "  (0, 73498)\t1\n",
      "  (0, 53839)\t1\n",
      "  (0, 71547)\t1\n",
      "  (0, 2238)\t1\n",
      "  (0, 19563)\t1\n",
      "  (0, 74324)\t1\n",
      "  (0, 66619)\t1\n",
      "  (0, 66387)\t1\n",
      "  (0, 59728)\t1\n",
      "  (0, 24608)\t1\n",
      "  (0, 41089)\t1\n",
      "  (0, 5502)\t1\n",
      "  (0, 2149)\t1\n",
      "  (0, 30670)\t1\n",
      "  (0, 57715)\t1\n",
      "  (0, 72088)\t1\n",
      "  (0, 14645)\t1\n",
      "  (0, 67324)\t1\n",
      "  (0, 22083)\t1\n",
      "  (0, 15153)\t1\n",
      "  (0, 22050)\t1\n",
      "  (0, 46957)\t1\n",
      "  (0, 73714)\t1\n",
      "  :\t:\n",
      "  (24999, 72246)\t1\n",
      "  (24999, 22056)\t1\n",
      "  (24999, 6334)\t1\n",
      "  (24999, 8583)\t1\n",
      "  (24999, 9881)\t1\n",
      "  (24999, 30646)\t1\n",
      "  (24999, 41513)\t1\n",
      "  (24999, 2662)\t1\n",
      "  (24999, 71159)\t1\n",
      "  (24999, 41798)\t1\n",
      "  (24999, 58613)\t1\n",
      "  (24999, 43995)\t1\n",
      "  (24999, 72784)\t1\n",
      "  (24999, 30118)\t1\n",
      "  (24999, 24536)\t1\n",
      "  (24999, 34585)\t1\n",
      "  (24999, 3258)\t1\n",
      "  (24999, 4465)\t1\n",
      "  (24999, 44164)\t1\n",
      "  (24999, 46050)\t1\n",
      "  (24999, 66562)\t1\n",
      "  (24999, 66339)\t1\n",
      "  (24999, 34683)\t1\n",
      "  (24999, 67125)\t1\n",
      "  (24999, 46680)\t1\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02', '020410', '029', '03', '04', '041', '05', '050', '06', '06th', '07', '08', '087', '089', '08th', '09', '0f', '0ne', '0r', '0s', '10', '100', '1000', '1000000', '10000000000000', '1000lb', '1000s', '1001', '100b', '100k', '100m', '100min', '100mph', '100s', '100th', '100x', '100yards', '101', '101st', '102', '102nd', '103', '104', '1040', '1040a', '1040s', '105', '1050', '105lbs', '106', '106min', '107', '108', '109', '10am', '10lines', '10mil', '10min', '10minutes', '10p', '10pm', '10s', '10star', '10th', '10x', '10yr', '11', '110', '1100', '11001001', '1100ad', '111', '112', '1138', '114', '1146', '115', '116', '117', '11f', '11m', '11th', '12', '120', '1200', '1200f', '1201', '1202', '123', '12383499143743701', '125', '125m', '127', '128', '12a', '12hr', '12m', '12mm', '12s', '12th', '13', '130', '1300', '1300s', '131', '1318', '132', '134', '135', '135m', '136', '137', '138', '139', '13k', '13s', '13th', '14', '140', '1408', '140hp', '1415', '142', '145', '1454', '146', '147', '1473', '149', '1492', '14a', '14ieme', '14s', '14th', '14yr', '14ème', '15', '150', '1500', '1500s', '150_worst_cases_of_nepotism', '150k', '150m', '151', '152', '153', '1547', '155', '156', '1561', '157', '158', '1594', '15mins', '15minutes', '15s', '15th', '16', '160', '1600', '1600s', '160lbs', '161', '1610', '163', '164', '165', '166', '1660s', '168', '169', '1692', '16ieme', '16k', '16mm', '16s', '16th', '16x9', '16ème', '16éme', '17', '170', '1700', '1700s', '1701', '171', '175', '177', '1775', '1780s', '1790s', '1794', '1798', '17million', '17th', '18', '180', '1800', '1800mph', '1800s', '1801', '1805', '1809', '180d', '1812', '1813', '18137', '1814', '1816', '1820', '1824', '183', '1830', '1832', '1836', '1837', '1838', '1839', '1840', '1840s', '1844', '1846', '1847', '185', '1850', '1850ies', '1850s', '1852', '1853', '1854', '1855', '1859', '1860', '1860s', '1861', '1862', '1863', '1864', '1865', '1870', '1870s', '1871', '1873', '1874', '1875', '1876', '188', '1880', '1880s', '1881', '1886', '1887', '1888', '1889', '188o', '1890', '1890s', '1892', '1893', '1894', '1895', '1896', '1897', '1898', '1899', '18a', '18s', '18th', '18year', '19', '190', '1900', '1900s', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1910s', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '192', '1920', '1920ies', '1920s', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1930ies', '1930s', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '193o', '194', '1940', '1940s', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1949er', '195', '1950', '1950s', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1960s', '1961', '1961s', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '197', '1970', '1970ies', '1970s', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '19796', '197o', '1980', '1980ies', '1980s', '1981', '1982', '1982s', '1983', '1983s', '1984', '1984ish', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19k', '19th', '19thc', '1am', '1and', '1d', '1h', '1h30', '1h40', '1h40m', '1h53', '1hour', '1hr', '1million', '1min', '1mln', '1o', '1s', '1st', '1ton', '1tv', '1½', '1ç', '20', '200', '2000', '20000', '20001', '2000ad', '2000s', '2001', '2002', '2003', '2004', '2004s', '2005', '2006', '2007', '2008', '2009', '200ft', '200th', '201', '2010', '2012', '2013', '2015', '2017', '2019', '2020', '2022', '2023', '2030', '2031', '2033', '2035', '2036', '2038', '204', '2040', '2044', '2046', '2047', '2050', '2053', '2054', '206', '2060', '2070', '2080', '209', '2090', '20c', '20ft', '20k', '20m', '20mins', '20minutes', '20mn', '20p', '20perr', '20s', '20th', '20ties', '20widow', '20x', '20year', '20yrs', '21', '210', '2100', '214', '215', '2151', '216', '21699', '21849889', '21849890', '21849907', '21st', '22', '220', '2200', '221', '2210', '22101', '222', '223', '225', '2257', '225mins', '227', '22d', '22h45', '22nd', '23', '230lbs', '230mph', '231', '232', '233', '236', '237', '23d', '23rd', '24', '240', '2400', '241', '242', '248', '2480', '249', '24m30s', '24th', '24years', '25', '250', '2500', '250000', '25million', '25mins', '25s', '25th', '25yo', '25yrs', '26', '260', '2600', '261k', '262', '2642', '269', '26th', '27', '270', '272', '273', '274', '275', '2772', '278', '27th', '27x41', '28', '280', '285', '28th', '29', '29th', '2am', '2d', '2fast', '2furious', '2h', '2h30', '2hour', '2hours', '2hr', '2hrs', '2in', '2inch', '2k', '2more', '2nd', '2oo4', '2oo5', '2pac', '2point4', '2s', '2x4', '30', '300', '3000', '300ad', '300c', '300lbs', '300mln', '3012', '303', '305', '30am', '30ish', '30k', '30lbs', '30min', '30mins', '30pm', '30s', '30something', '30th', '30ties', '31', '3199', '31st', '32', '3200', '320x180', '32lb', '32nd', '33', '330am', '330mins', '332960073452', '336th', '33m', '34', '345', '3462', '34th', '35', '350', '3500', '3516', '356', '357', '35c', '35mins', '35mm', '35pm', '35th', '35yr', '36', '360', '365', '36th', '37', '370', '372', '378', '38', '38k', '38th', '39', '395', '39th', '3am', '3bs', '3d', '3dvd', '3k', '3lbs', '3m', '3mins', '3p', '3p0', '3pm', '3po', '3rd', '3rds', '3th', '3who', '3x5', '3yrs', '40', '400', '4000', '401k', '405', '409', '40am', '40min', '40mins', '40mph', '40s', '40th', '41', '42', '420', '425', '428', '42nd', '43', '430', '44', '440', '442nd', '44c', '44yrs', '45', '450', '4500', '451', '454', '45am', '45min', '45mins', '45s', '46', '465', '469', '47', '475', '477', '47s', '48', '480m', '480p', '48hrs', '49', '498', '49th', '4am', '4cylinder', '4d', '4eva', '4ever', '4f', '4h', '4hrs', '4k', '4kids', '4m', '4o', '4pm', '4th', '4w', '4ward', '4x', '4x4', '50', '500', '5000', '500000', '500ad', '500db', '500lbs', '502', '50c', '50ft', '50ies', '50ish', '50k', '50min', '50mins', '50s', '50th', '50usd', '51', '51b', '51st', '52', '5200', '5250', '529', '52s', '53', '53m', '54', '5400', '540i', '54th', '55', '5539', '555', '55th', '56', '57', '571', '576', '578', '57d', '58', '58th', '59', '598947', '59th', '5hrs', '5ive', '5kph', '5million', '5min', '5mins', '5s', '5seconds', '5th', '5x', '5x5', '5years', '5yo', '5yrs', '60', '600', '6000', '607', '608', '60ies', '60ish', '60mph', '60s', '60th', '60ties', '61', '618', '62', '6200', '62229249', '63', '637', '63rd', '64', '65', '65m', '66', '660', '666', '66er', '67', '6723', '67th', '68', '68th', '69', '69th', '6am', '6b', '6ft', '6hours', '6k', '6million', '6pm', '6th', '6wks', '6yo', '70', '700', '701', '707', '70ies', '70m', '70mm', '70s', '70th', '71', '713', '72', '72nd', '73', '7300', '735', '737', '74', '740', '740il', '747', '747s', '74th', '75', '750', '75054', '75c', '75m', '76', '7600', '77', '78', '788', '78rpm', '79', '79th', '7days', '7even', '7eventy', '7ft', '7ish', '7mm', '7th', '7½th', '80', '800', '8000', '80ies', '80ish', '80min', '80s', '80yr', '81', '817', '819', '82', '820', '8217', '8230', '83', '84', '849', '84f', '84s', '85', '850', '850pm', '86', '86s', '87', '8700', '8763', '878', '87minutes', '88', '88min', '89', '89or', '89s', '8bit', '8ftdf', '8k', '8mm', '8o', '8p', '8pm', '8star', '8th', '8u', '8½', '90', '900', '9000', '90210', '905', '90c', '90ish', '90min', '90mins', '90s', '91', '911', '914', '917', '92', '921', '92fs', '92nd', '93', '937', '94', '9484', '94s', '94th', '95', '950', '95th', '96', '97', '970', '974th', '978', '98', '987', '98minutes', '99', '998', '999', '9999', '99cents', '99p', '99½', '9_', '9am', '9as', '9do', '9ers', '9is', '9lbs', '9mm']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the feature matrix done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split\n",
    "\n",
    "* Train data - all training based on it (this includes the vectorizer!)\n",
    "* Development data - set the parameters\n",
    "* Test data - used for nothing during training, produce final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, dev_texts, train_labels, dev_labels=train_test_split(texts,labels,test_size=0.2)\n",
    "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\n",
    "feature_matrix_train=vectorizer.fit_transform(train_texts)\n",
    "feature_matrix_dev=vectorizer.transform(dev_texts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 68295)\n",
      "(5000, 68295)\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix_train.shape)\n",
    "print(feature_matrix_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier train\n",
    "\n",
    "* Let us try the venerable, if a bit outdated SVM\n",
    "* Linear SVM for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.0005, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "classifier=sklearn.svm.LinearSVC(C=0.0005,verbose=1)\n",
    "classifier.fit(feature_matrix_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "* For a quick test we can use the .score() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV 0.8666\n",
      "TRAIN 0.8954\n"
     ]
    }
   ],
   "source": [
    "print(\"DEV\",classifier.score(feature_matrix_dev, dev_labels))\n",
    "print(\"TRAIN\",classifier.score(feature_matrix_train, train_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try varying the C value and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'pos' 'pos' ... 'pos' 'pos' 'pos']\n",
      "[[2128  368]\n",
      " [ 299 2205]]\n",
      "0.8666\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "predictions_dev=classifier.predict(feature_matrix_dev)\n",
    "print(predictions_dev)\n",
    "print(sklearn.metrics.confusion_matrix(dev_labels,predictions_dev))\n",
    "print(sklearn.metrics.accuracy_score(dev_labels,predictions_dev))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
