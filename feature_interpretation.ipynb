{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementary feature interpretation\n",
    "\n",
    "* Linear classifier learns a weight for every feature\n",
    "* This weight (to a degree) correlates with the importance of this feature\n",
    "* Study the attributes of a trained `LinearSVC` here https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "* `coef_` seems to be the right choice    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier weights: [[-0.00424805 -0.00873877 -0.0004353  ...  0.          0.00021581\n",
      "  -0.00089298]]\n",
      "their shape (1, 68408)\n"
     ]
    }
   ],
   "source": [
    "#Load the previously saved model\n",
    "import pickle\n",
    "import sklearn\n",
    "with open(\"saved_model.pickle\",\"rb\") as f:\n",
    "    classifier,vectorizer=pickle.load(f)\n",
    "    \n",
    "print(\"classifier weights:\",classifier.coef_)\n",
    "print(\"their shape\",classifier.coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will need to know which weight corresponds to which feature\n",
    "* The vectorizer has this information\n",
    "* Study the attributes here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "* `vocabulary` seems to be the right choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('does', 17783), ('anyone', 3363), ('know', 33743), ('where', 66517), ('can', 9444), ('see', 53555), ('or', 43034), ('download', 18197), ('the', 60663), ('what', 66467), ('like', 35430), ('about', 1154), ('you', 67946), ('season', 53449), ('episodes', 20358), ('in', 30186), ('internet', 31245), ('because', 5895), ('would', 67400), ('die', 16770), ('to', 61376), ('them', 60696), ('and', 2948), ('here', 28058), ('germany', 24969), ('there', 60745), ('won', 67195), ('be', 5778), ('shown', 54752), ('on', 42826)] ...\n"
     ]
    }
   ],
   "source": [
    "# print(vectorizer.vocabulary_) # beware! large print\n",
    "print(list(vectorizer.vocabulary_.items())[:30],\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The vocabulary is a dictionary: feature -> index\n",
    "* We will need it the other way, i.e. we will need to ask using an index, and get the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reverse the dictionary\n",
    "index2feature={}\n",
    "for feature,idx in vectorizer.vocabulary_.items():\n",
    "    assert idx not in index2feature #This really should hold\n",
    "    index2feature[idx]=feature\n",
    "#Now we can query index2feature to get the feature names as we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need now to sort the classifier weights\n",
    "* ...and keep the information about which features (indices) they correspond to\n",
    "* So a simple `sort()` does not cut it, we would not keep the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst\n",
      "bad\n",
      "waste\n",
      "awful\n",
      "boring\n",
      "poor\n",
      "nothing\n",
      "terrible\n",
      "worse\n",
      "dull\n",
      "no\n",
      "unfortunately\n",
      "stupid\n",
      "ridiculous\n",
      "poorly\n",
      "supposed\n",
      "minutes\n",
      "annoying\n",
      "script\n",
      "money\n",
      "avoid\n",
      "disappointment\n",
      "instead\n",
      "horrible\n",
      "disappointing\n",
      "fails\n",
      "plot\n",
      "save\n",
      "lame\n",
      "crap\n",
      "----------------------------------------------------\n",
      "great\n",
      "excellent\n",
      "perfect\n",
      "best\n",
      "wonderful\n",
      "amazing\n",
      "loved\n",
      "love\n",
      "fun\n",
      "today\n",
      "favorite\n",
      "well\n",
      "highly\n",
      "enjoyed\n",
      "beautiful\n",
      "enjoyable\n",
      "definitely\n",
      "brilliant\n",
      "superb\n",
      "job\n",
      "especially\n",
      "simple\n",
      "also\n",
      "fantastic\n",
      "enjoy\n",
      "very\n",
      "liked\n",
      "still\n",
      "dvd\n",
      "makes\n"
     ]
    }
   ],
   "source": [
    "# Solution 1:\n",
    "\n",
    "# make a list of (weight, index), sort it\n",
    "lst=[]\n",
    "for idx,weight in enumerate(classifier.coef_[0]):\n",
    "    lst.append((weight,idx))\n",
    "lst.sort() #sort\n",
    "\n",
    "#Print first few and last few\n",
    "for weight,idx in lst[:30]: #first 30 (ie lowest weight)\n",
    "    print(index2feature[idx])\n",
    "print(\"----------------------------------------------------\")\n",
    "#Take the last 30 (lst[-30:]) but these now come from weakest to strongest\n",
    "#so reverse the list using [::-1]\n",
    "for weight,idx in lst[-30:][::-1]:\n",
    "    print(index2feature[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67388  5009 66033 ... 44948 20991 26119]\n",
      "worst\n",
      "bad\n",
      "waste\n",
      "awful\n",
      "boring\n",
      "poor\n",
      "nothing\n",
      "terrible\n",
      "worse\n",
      "dull\n",
      "no\n",
      "unfortunately\n",
      "stupid\n",
      "ridiculous\n",
      "poorly\n",
      "supposed\n",
      "minutes\n",
      "annoying\n",
      "script\n",
      "money\n",
      "avoid\n",
      "disappointment\n",
      "instead\n",
      "horrible\n",
      "disappointing\n",
      "fails\n",
      "plot\n",
      "save\n",
      "lame\n",
      "crap\n",
      "-------------------------------\n",
      "great\n",
      "excellent\n",
      "perfect\n",
      "best\n",
      "wonderful\n",
      "amazing\n",
      "loved\n",
      "love\n",
      "fun\n",
      "today\n",
      "favorite\n",
      "well\n",
      "highly\n",
      "enjoyed\n",
      "beautiful\n",
      "enjoyable\n",
      "definitely\n",
      "brilliant\n",
      "superb\n",
      "job\n",
      "especially\n",
      "simple\n",
      "also\n",
      "fantastic\n",
      "enjoy\n",
      "very\n",
      "liked\n",
      "still\n",
      "dvd\n",
      "makes\n"
     ]
    }
   ],
   "source": [
    "# Solution #2\n",
    "# Numpy can help us\n",
    "# argsort gives a sequence of indices that sort an array\n",
    "import numpy\n",
    "\n",
    "indices=numpy.argsort(classifier.coef_[0])\n",
    "print(indices)\n",
    "for idx in indices[:30]:\n",
    "    print(index2feature[idx])\n",
    "print(\"-------------------------------\")\n",
    "for idx in indices[::-1][:30]: #you can also do it the other way round, reverse, then pick\n",
    "    print(index2feature[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This seems to work like charm!\n",
    "* We can sample the features across the range to get some further idea\n",
    "* let's take every 100th feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst, then, so, contrived, talents, bear, yawn, wretched, bob, shark, amounts, cries, interminable, invisible, invested, alligator, vince, selfish, undermines, fisherman, trendy, firstly, stunk, thankless, ramsey, eggs, crosby, dead, newcomer, dreamgirls, del, celebrated, drawl, gimmicky, tolerate, boringly, come, aime, surrealism, gods, tilly, predicted, nayland, jerking, actively, waldemar, bottle, compounded, hazy, chiles, jung, morley, puddle, leaded, bombers, marionettes, upscale, remaking, variously, apropos, reno, ashford, indictment, parishioners, crammed, bradbury, newborn, shifty, titter, misanthropic, germ, cornel, oppressively, impervious, commands, ding, danyael, bueller, luke, darkling, chesty, gentleness, fireballs, cassetti, necklines, nuyoricans, annoyance, dyes, lyndon, benno, dint, wading, abuses, drips, increases, royally, ez, deckchair, quayle, misfit, mirrored, christmastime, psicoanalitical, patting, liquer, griffin, unworthy, nelli, scriptwriter, mcinally, androids, powermaster, whe, mortician, exterminated, playtime, incidental, cro, ariauna, errands, eff, fist, drunkenly, seaward, activity, unremarkably, daryll, kristel, talky, deployment, flix, mimic, effusive, scorcese, benvolio, transcript, formless, zoological, surmise, contemperaneous, engel, zzzzzzzzzzzz, masculin, stratofreighter, cribbing, unearthly, jeopardizes, refueled, nationalism, radiohead, threepenny, monsteroid, cornelia, hy, pooh, perfume, brake, priestesses, concise, midlands, cruelty, ryoo, seashell, demeans, deluding, morena, gwyenths, shoebat, vill, absconding, drenched, londoners, isco, pavillion, flagwaving, tlb, reforming, attentive, pompadour, whitlow, spacecrafts, henkin, partially, prepubescent, foursome, plumage, crighton, puff, vulpine, snatchers, aborted, trongard, yowling, popsicles, splayed, gennosuke, rivera, sloshing, tonne, byers, hydrogen, selfishness, nekeddo, thialnd, uped, notary, sorta, samedi, pockets, 500db, emailed, ce3k, bombardment, partaking, weirded, psychobilly, bantering, 1hr, rhames, nicotero, margotta, retrospect, titallition, minimize, upstanding, emu, dandylion, trashin, nahin, ow, doncha, cabbie, reshipping, risibly, basia, anywayz, penetrate, neglecting, hammill, located, suicune, excitied, bump, jaglom, dizzying, pinetrees, noelle, airspace, naha, livingstone, doreen, silvermann, canceling, nicolette, stashes, undeserving, grievance, typographical, aaawwwwnnn, dufus, sione, lazar, fbp, hydrate, culpas, barabar, procrastinate, decibels, invincibility, lances, bertie, abreast, schwarzmann, ululating, wg, prowled, poliwhirl, dietrickson, incarceration, illuminates, overdeveloped, vit, golovanov, fonts, oily, mischief, compositely, discards, alkie, swimsuit, agnisakshi, microcuts, javo, coccio, scud, unappetising, kaspar, arrant, underuse, consecutive, cleverer, cupboard, chooser, gassed, bulldozer, vauxhall, environmentalists, thereof, discouragement, hinged, anthropophagus, mouthburster, sinnui, sartor, teuton, teleporter, mohawk, ostentation, traditionalism, outrunning, salmaan, lene, golfing, sagramore, stuccoed, dubey, dumass, mancoy, gauging, narayan, sputtered, matchup, lowber, lundegaard, longshanks, mucking, nob, suicidally, surronding, absolutelly, galilee, physicallity, verbosely, alums, fennie, dandelions, jeopardised, uttter, valalola, irreverently, woodworking, quisessential, bohemia, bookstores, hamada, hinter, ravel, annelle, imam, dewitt, udit, goy, esperance, roiling, panjabi, tsau, jiggy, cryptologist, birnam, traipses, gastronomic, lumped, architected, unkill, miniatures, overcompensating, mchugh, impalements, dependable, naudets, ruffianly, crutch, predators, leoncavallo, unkindness, investigatory, merquise, frenches, bargains, everlastingly, aristides, bankable, vern, incline, sergo, gumshoe, diry, boaz, turnstiles, barbeau, cauldron, conciseness, slogging, viver, cappomaggi, slowmotion, adheres, ttws, diahnn, raleigh, piovani, meera, fleadh, telescoping, activision, implants, adoree, certificate, reopen, windbag, giuseppe, detonate, sharpening, breakthroughs, pillage, columbus, rox, pulpy, katee, definate, camouflages, log, paddy, deangelo, adèle, kansan, foregoes, brotherly, britons, rigets, vest, gony, manchurian, patriots, ret, désirée, yrs, doves, cineliterate, actores, bernal, calchas, backett, seeded, driller, belisario, alfred, sumire, turbulence, plagiaristic, goldoni, mandy62, naïveté, golino, pouch, fils, withstood, tomorrows, disbanded, muckerji, yu, gelatin, topsy, préte, interceding, sílvia, gandhian, cg, ranft, appointed, marzipan, culls, lourenço, functionality, fye, sleek, rhein, generatively, endangered, impalement, heterai, sisto, leatrice, bites, 20yrs, sleepovers, chompers, smallweed, researcher, affiliate, negating, 237, foreignness, institutionalized, births, relived, weered, affirming, viscous, emannuelle, unfilmed, penislized, intriguded, sundae, newmar, synonamess, chieftain, volpi, ghraib, donell, angola, junkies, ligabue, yootha, slamdunk, benton, parodist, reflective, boried, jutras, austens, earthiness, marine, battlefields, wabbit, grows, cavett, warburton, arriviste, nabakov, pessimism, parachute, halflings, oddest, cycles, anatomical, populous, wags, aniston, slayings, cogs, levers, unquestionable, strick, shrine, srbljanovic, cirus, corby, elusively, notte, battre, dismissal, carbide, squibs, fedar, imperiously, caultron, expectancy, limbs, patches, bug, inability, frechette, sicko, ridgway, nightsheet, jugular, demotes, reiterate, dutiful, confederation, peninsula, roxbury, woodland, kissinger, tulkinghorn, pâquerette, escher, obligation, abduction, perceptively, nickleby, rewinding, dispensed, dub, margarethe, quirk, rotoscoping, creeper, barr, crescendo, explosive, pecker, vigilantism, keighley, telltale, lawn, brimley, yale, blimp, ponds, patterns, 27, brassed, duryea, superiority, troupe, ambush, immoral, cena, yeaworth, probes, serialized, laundering, oversight, headlines, forsyth, crawled, enlists, bison, resisted, nunez, legacy, contend, toronto, standouts, investigates, advertise, sheeta, resistant, mercenary, dam, midler, plate, unflinching, teri, offenders, distributor, booker, activists, trait, skipper, perpetually, ton, concentrated, revolutionary, cruiserweight, helmet, tourist, meddling, awed, unravel, superhero, culp, bernsen, nightclub, atlantis, scan, dress, charismatic, mannerisms, distant, packs, bud, british, burns, roll, masterful, awe, deals, originally, due, french, jack, love, "
     ]
    }
   ],
   "source": [
    "indices=numpy.argsort(classifier.coef_[0])\n",
    "\n",
    "for idx in indices[::100]:\n",
    "    print(index2feature[idx],end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What have we learned?\n",
    "* Most of the features seem to form a mass without a strong correlation with the sentiment\n",
    "* Only the very extremes of the list seem to be strongly sentiment-biased\n",
    "* This can be tested by looking at the distribution of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafa27b5bca548dabcb52487ef72a22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5103e1390bfb499984d4f8b76f4c07c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "\n",
    "plt.figure()\n",
    "y = np.sort(classifier.coef_[0])\n",
    "plt.plot(y)\n",
    "plt.show()\n",
    "\n",
    "#let's look at the beginning, see how fast the weights drop\n",
    "plt.figure()\n",
    "y=np.sort(classifier.coef_[0])\n",
    "plt.plot(y[:1000])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
