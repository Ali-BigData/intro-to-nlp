{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "nn_classifier_features.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/intro-to-nlp/blob/master/nn_classifier_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqnaDSDTUYd4"
      },
      "source": [
        "# What is the network learning?\n",
        "\n",
        "* We can gain some intuition in what the network is learning\n",
        "* Especially fruitful will turn out to look at the weights of the hidden layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J3M2GVVZRy9",
        "outputId": "9018cfde-1343-495b-94da-b3e917ae319a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAz55Z0vUYeL",
        "outputId": "bde4f4c0-08b4-46aa-b7b2-0de46506d252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import pickle\n",
        "from keras.models import Model, model_from_json\n",
        "\n",
        "\n",
        "def load_model(model_name):\n",
        "\n",
        "    with open(model_name+\".model.json\", \"rt\") as f:\n",
        "        model=model_from_json(f.read())\n",
        "    model.load_weights(model_name+\".weights.h5\")\n",
        "    \n",
        "    with open(model_name+\".encoders.pickle\",\"rb\") as f:\n",
        "        label_encoder,vectorizer=pickle.load(f)\n",
        "    \n",
        "    return model,label_encoder,vectorizer\n",
        "\n",
        "#Model saved to drive at the end of the nn_bow_classifier notebook!\n",
        "model,label_encoder,vectorizer=load_model(\"/content/drive/MyDrive/intro-to-nlp/models-2021/imdb_bow\")\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.functional.Functional object at 0x7f503f3bbf98>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NwNMz6iUYeS",
        "outputId": "ecd4d0e7-c467-477c-acd0-06eb987ee2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model,show_shapes=True,show_layer_names=False,dpi=65).create(prog='dot', format='svg'))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"200pt\" viewBox=\"0.00 0.00 257.00 221.00\" width=\"232pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.9028 .9028) rotate(0) translate(4 217)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-217 253,-217 253,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139982634996848 -->\n<g class=\"node\" id=\"node1\">\n<title>139982634996848</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 249,-212.5 249,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40\" y=\"-185.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"80,-166.5 80,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"80,-189.5 138,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"138,-166.5 138,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-197.3\">[(None, 68351)]</text>\n<polyline fill=\"none\" points=\"138,-189.5 249,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-174.3\">[(None, 68351)]</text>\n</g>\n<!-- 139983477947024 -->\n<g class=\"node\" id=\"node2\">\n<title>139983477947024</title>\n<polygon fill=\"none\" points=\"18.5,-83.5 18.5,-129.5 230.5,-129.5 230.5,-83.5 18.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"44.5\" y=\"-102.8\">Dense</text>\n<polyline fill=\"none\" points=\"70.5,-83.5 70.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"70.5,-106.5 128.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"128.5,-83.5 128.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.5\" y=\"-114.3\">(None, 68351)</text>\n<polyline fill=\"none\" points=\"128.5,-106.5 230.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.5\" y=\"-91.3\">(None, 200)</text>\n</g>\n<!-- 139982634996848&#45;&gt;139983477947024 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139982634996848-&gt;139983477947024</title>\n<path d=\"M124.5,-166.3799C124.5,-158.1745 124.5,-148.7679 124.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"128.0001,-139.784 124.5,-129.784 121.0001,-139.784 128.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139982634992864 -->\n<g class=\"node\" id=\"node3\">\n<title>139982634992864</title>\n<polygon fill=\"none\" points=\"26,-.5 26,-46.5 223,-46.5 223,-.5 26,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"78,-.5 78,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"78,-23.5 136,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"136,-.5 136,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.5\" y=\"-31.3\">(None, 200)</text>\n<polyline fill=\"none\" points=\"136,-23.5 223,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.5\" y=\"-8.3\">(None, 2)</text>\n</g>\n<!-- 139983477947024&#45;&gt;139982634992864 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139983477947024-&gt;139982634992864</title>\n<path d=\"M124.5,-83.3799C124.5,-75.1745 124.5,-65.7679 124.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"128.0001,-56.784 124.5,-46.784 121.0001,-56.784 128.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJew8Ml2UYeT"
      },
      "source": [
        "* This is our model (watch out, Keras plots models from top to bottom)\n",
        "* Hidden layer has N nodes with 74849 inputs each, one input for one word in the vocabulary\n",
        "* We can also look at it the other way around: each word is assigned one weight for each hidden layer node\n",
        "* So each word is seen by the network as a N-dimensional vector\n",
        "* But what are these vectors? What properties should they have? Let us find out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAJDRxWqUYeT",
        "outputId": "200b42b2-cbff-49cb-de5c-17bc705d78c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "learned_weights=model.layers[1].get_weights()[0]\n",
        "learned_weights.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68351, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeX_vtF5UYeU"
      },
      "source": [
        "* A good and easy way to explore the vectors is through their neighborhood\n",
        "* This is easy to implement so let's try"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKMo1-I7UYeV",
        "outputId": "dc57f16e-f00b-4a6c-91a8-7771af67e7c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy\n",
        "\n",
        "def nearest(word,learned_weights,vectorizer):\n",
        "    inverse_vocab=dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
        "    word_idx=vectorizer.vocabulary_[word]\n",
        "    word_vector=learned_weights[word_idx]\n",
        "    x=numpy.linalg.norm(word_vector-learned_weights,axis=-1)\n",
        "    nearest=numpy.argsort(x)\n",
        "    for idx in nearest[:30]:\n",
        "        print(inverse_vocab[idx], end=\", \")\n",
        "    print()\n",
        "    \n",
        "nearest(\"bad\",learned_weights,vectorizer)\n",
        "nearest(\"terrible\",learned_weights,vectorizer)\n",
        "\n",
        "nearest(\"great\",learned_weights,vectorizer)\n",
        "nearest(\"enjoyable\",learned_weights,vectorizer)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bad, wasted, endless, unwatchable, forwarding, embarrassed, dire, sooooo, puerile, turkey, unfunny, lackluster, telekinesis, stinks, mercifully, mentions, laughable, horrible, od, wasting, stinker, behave, unless, miscast, uninteresting, ashamed, annoying, meow, baseless, evel, \n",
            "terrible, worse, leaning, mess, mcgovern, lacks, dull, tashan, lousy, moonlighting, alleged, disappointing, rochon, completists, boredom, soap, boring, awful, wasting, mercifully, ardent, wuhrer, lackluster, avoid, unfunny, badly, dir, wasted, turkey, horrible, \n",
            "great, sailor, trained, entertaining, bewilderment, surpasses, hawke, endearingly, tripping, pleasantly, brisk, masterson, powerful, beautifully, wonderful, laputa, symbols, elephant, gently, novak, krell, cheer, fantastically, edie, elmer, drake, intense, da, packed, rosalind, \n",
            "enjoyable, flawless, relaxed, ankle, excellently, liu, braveheart, grander, sentenced, fantastic, rare, superbly, haunting, reveal, underrated, masterson, lotus, amazing, johnston, symbols, funniest, prelude, fantastically, treaty, delightfully, perfect, elephant, reservations, ramones, brisk, \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}